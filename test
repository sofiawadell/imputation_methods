#### FACTOR ENCODING 
import pandas as pd
import numpy as np
from datasets import datasets

dataset = "credit"
missingness = 30

# df[col] = pd.Categorical(df[col], categories=df[col].unique(), ordered=False)

# Find categorical and target column
cat_cols = datasets[dataset]["cat_cols"]
target_col = datasets[dataset]["target"]

# Concatenate complete datasets
filename_train_complete = 'train_data/{}_train.csv'.format(dataset)
train_data_complete = pd.read_csv(filename_train_complete)

filename_test_complete = 'test_data/{}_test.csv'.format(dataset)
test_data_complete = pd.read_csv(filename_test_complete)

full_data_complete = pd.concat([train_data_complete, test_data_complete], axis=0)
target_col_full_data_complete = full_data_complete[target_col]
full_data_complete = full_data_complete.drop(target_col, axis=1) # Full data without missingness

# Concatenate datasets with missingness
filename_train_x = 'train_data/{}_train_{}.csv'.format(dataset, missingness)
train_data_x = pd.read_csv(filename_train_x)

filename_test_x = 'test_data/{}_test_{}.csv'.format(dataset, missingness)
test_data_x = pd.read_csv(filename_test_x)

full_data_x = pd.concat([train_data_x, test_data_x], axis=0)
target_col_full_data_x = full_data_x[target_col]
full_data_x = full_data_x.drop(target_col, axis=1) # Full data with missingness without target column

# Create copy of dataframes
df_full_data_complete = full_data_complete.copy()
df_full_data_x = full_data_x.copy()

# Loop through each categorical column and apply dummy encoding
for col in cat_cols:
    col_encoded_name = col + '_encoded'
    unique_values = df_full_data_complete[col].dropna().unique()  # exclude np.nan values
    mapping = dict(zip(unique_values, range(len(unique_values))))
    df_full_data_complete[col_encoded_name] = df_full_data_complete[col].replace(mapping)
    df_full_data_complete[col_encoded_name] = df_full_data_complete[col_encoded_name].replace({np.nan: np.nan})

    unique_values = df_full_data_x[col].dropna().unique()  # exclude np.nan values
    mapping = dict(zip(unique_values, range(len(unique_values))))
    df_full_data_x[col_encoded_name] = df_full_data_x[col].replace(mapping)
    df_full_data_x[col_encoded_name] = df_full_data_x[col_encoded_name].replace({np.nan: np.nan})

# Add back the target column
df_full_data_x[target_col] = target_col_full_data_x
df_full_data_complete[target_col] = target_col_full_data_complete

# Split back into training and test
train_data_complete, test_data_complete = np.vsplit(df_full_data_complete, [len(train_data_complete)])
train_data_x, test_data_x = np.vsplit(df_full_data_x, [len(train_data_x)])

print(train_data_x)


# # Save to CSV
# filename_train_complete = 'factor_train_data/factor_encode_{}_train.csv'.format(dataset)
# train_data_complete.to_csv(filename_train_complete, index=False)
# filename_test_complete = 'factor_test_data/factor_encode_{}_test.csv'.format(dataset)
# test_data_complete.to_csv(filename_test_complete, index=False)

# filename_train_x = 'factor_train_data/factor_encode_{}_train_{}.csv'.format(dataset, missingness)
# train_data_x.to_csv(filename_train_x, index=False)
# filename_test_x = 'factor_test_data/factor_encode_{}_test_{}.csv'.format(dataset, missingness)
# test_data_x.to_csv(filename_test_x, index=False)

# print(test_data_x.shape)
# print(train_data_x.shape)

# print(test_data_complete.shape)
# print(train_data_complete.shape)





#### DRAFT

# # Perform factor encoding on the column 
#     encoded_col = col + '_encoded'
#     categories = df_full_data_x[col].dropna().unique()
#     df_full_data_x[encoded_col] = pd.Series(pd.Categorical(df_full_data_x[col], categories=categories), dtype="category")
#     encoded_cols[col] = encoded_col
#     df_full_data_x.drop(columns=[col], inplace=True)

#     encoded_col = col + '_encoded'
#     categories = df_full_data_complete[col].dropna().unique()
#     df_full_data_complete[encoded_col] = pd.Series(pd.Categorical(df_full_data_complete[col], categories=categories), dtype="category")
#     encoded_cols[col] = encoded_col
#     df_full_data_complete.drop(columns=[col], inplace=True)

                    
# # Rename the encoded columns    
# df_full_data_complete.rename(columns=encoded_cols, inplace=True)
# df_full_data_x.rename(columns=encoded_cols, inplace=True)


# ######### MICE
# from data_loader import data_loader_factor
# from data_loader import data_loader_full 
# import numpy as np 
# from utils import normalization
# import pandas as pd

# import rpy2.robjects as ro
# from rpy2.robjects.packages import importr
# from rpy2.robjects import pandas2ri
# from rpy2.robjects.conversion import localconverter

# data_name = 'credit'
# miss_rate = 0.1

# # # [TBU] Find max/min values for entire data set to enable normalization for numerical variables 
# # full_data = data_loader_full(data_name, miss_rate)
# # norm_data, norm_parameters = normalization(full_data)

# # Load data, introduce missingness & dummy encode 
# train_data_x, train_miss_data_x, test_data_x, test_miss_data_x = data_loader_factor(data_name, miss_rate) 

# # Define mask matrix
# mask_train = 1-np.isnan(train_miss_data_x)
# mask_test = 1-np.isnan(test_miss_data_x)

# # # [TBU] Normalize all data sets using the norm_parameters 
# # data = [train_data_x,train_miss_data_x, test_data_x, test_miss_data_x]
# # train_data_norm_x, train_miss_data_norm_x, test_data_norm_x, test_miss_data_norm_x = [normalization(d,norm_parameters) for d in data]

# # Before normalization is implemented - to be removed afterwards 
# train_data_norm_x, train_miss_data_norm_x, test_data_norm_x, test_miss_data_norm_x = train_data_x, train_miss_data_x, test_data_x, test_miss_data_x

# # Load packages in R 
# utils = importr('utils')
# mice = importr('mice')
# tidyr = importr('tidyr')
# complete = ro.r['complete']

# pandas2ri.activate()

# # convert the pandas dataframe to a R dataframe
# with localconverter(ro.default_converter + pandas2ri.converter):
#    R_train_miss_data_norm_x = ro.conversion.py2rpy(train_miss_data_norm_x) 

# tmp = mice.mice(R_train_miss_data_norm_x, m=5)

# df_imputated = complete(tmp, 1)